{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspired by:**\n",
    "- https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "- https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb\n",
    "- https://www.kaggle.com/therohk/million-headlines/data?select=abcnews-date-text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivianho/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "documents = data[['headline_text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(documents))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vivianho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization:  Segmenting a document into atomic elements\n",
    "2. Words that have fewer than 3 characters are removed.\n",
    "3. Remove stop words: 250-300 most common words in English account for 50% or more of a given text.\n",
    "4. Stemming: producing morphological variants of a root/base word\n",
    "5. Lemmatization:Words are lemmatized â€” words in third person are changed to first person and verbs in past and future tenses are changed into present. Converting the word to its meaningful base form considering the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents.index == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 words of the dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in\n",
    "# 1. less than 15 documents (absolute number) or\n",
    "# 2. more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "# 3. after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(162, 1), (240, 1), (292, 1), (589, 1), (838, 1), (3570, 1), (3571, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 1) govt\n",
      "(240, 1) group\n",
      "(292, 1) vote\n",
      "(589, 1) local\n",
      "(838, 1) want\n",
      "(3570, 1) compulsori\n",
      "(3571, 1) ratepay\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(bow_doc_4310[i], dictionary[bow_doc_4310[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226258"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5842699484464488),\n",
      " (1, 0.38798859072167835),\n",
      " (2, 0.5008422243250992),\n",
      " (3, 0.5071987254965034)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, \n",
    "                                       passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.026*\"death\" + 0.025*\"charg\" + 0.025*\"case\" + 0.024*\"court\" + 0.021*\"polic\" + 0.020*\"murder\" + 0.015*\"alleg\" + 0.012*\"trial\" + 0.012*\"arrest\" + 0.012*\"face\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"news\" + 0.020*\"market\" + 0.018*\"world\" + 0.017*\"women\" + 0.015*\"final\" + 0.015*\"australian\" + 0.014*\"island\" + 0.012*\"return\" + 0.011*\"street\" + 0.011*\"fall\"\n",
      "Topic: 2 \n",
      "Words: 0.051*\"coronavirus\" + 0.028*\"covid\" + 0.024*\"live\" + 0.021*\"nation\" + 0.021*\"coast\" + 0.016*\"restrict\" + 0.015*\"water\" + 0.013*\"gold\" + 0.011*\"plan\" + 0.010*\"park\"\n",
      "Topic: 3 \n",
      "Words: 0.038*\"sydney\" + 0.025*\"polic\" + 0.021*\"crash\" + 0.020*\"adelaid\" + 0.019*\"die\" + 0.015*\"miss\" + 0.012*\"break\" + 0.011*\"drug\" + 0.011*\"driver\" + 0.010*\"road\"\n",
      "Topic: 4 \n",
      "Words: 0.037*\"year\" + 0.031*\"melbourn\" + 0.022*\"open\" + 0.021*\"canberra\" + 0.017*\"jail\" + 0.015*\"work\" + 0.014*\"high\" + 0.014*\"life\" + 0.013*\"interview\" + 0.013*\"offic\"\n",
      "Topic: 5 \n",
      "Words: 0.029*\"govern\" + 0.019*\"health\" + 0.019*\"school\" + 0.017*\"help\" + 0.016*\"chang\" + 0.015*\"feder\" + 0.013*\"indigen\" + 0.012*\"state\" + 0.012*\"coronavirus\" + 0.012*\"fund\"\n",
      "Topic: 6 \n",
      "Words: 0.070*\"australia\" + 0.045*\"trump\" + 0.025*\"donald\" + 0.017*\"elect\" + 0.016*\"border\" + 0.015*\"busi\" + 0.015*\"peopl\" + 0.014*\"accus\" + 0.013*\"say\" + 0.012*\"scott\"\n",
      "Topic: 7 \n",
      "Words: 0.042*\"queensland\" + 0.033*\"victoria\" + 0.022*\"bushfir\" + 0.022*\"hous\" + 0.014*\"time\" + 0.013*\"royal\" + 0.013*\"west\" + 0.011*\"guilti\" + 0.011*\"commiss\" + 0.011*\"farmer\"\n",
      "Topic: 8 \n",
      "Words: 0.030*\"china\" + 0.025*\"test\" + 0.019*\"south\" + 0.016*\"coronavirus\" + 0.013*\"north\" + 0.012*\"australian\" + 0.012*\"rural\" + 0.011*\"presid\" + 0.011*\"train\" + 0.011*\"minist\"\n",
      "Topic: 9 \n",
      "Words: 0.025*\"call\" + 0.025*\"tasmania\" + 0.020*\"rise\" + 0.019*\"victorian\" + 0.017*\"morrison\" + 0.017*\"tasmanian\" + 0.015*\"million\" + 0.015*\"farm\" + 0.011*\"program\" + 0.011*\"claim\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.010*\"christma\" + 0.010*\"stori\" + 0.009*\"sport\" + 0.007*\"violenc\" + 0.007*\"domest\" + 0.007*\"award\" + 0.006*\"septemb\" + 0.006*\"celebr\" + 0.006*\"australian\" + 0.006*\"dollar\"\n",
      "Topic: 1 Word: 0.014*\"murder\" + 0.013*\"court\" + 0.012*\"charg\" + 0.010*\"alleg\" + 0.008*\"friday\" + 0.008*\"jail\" + 0.008*\"drug\" + 0.008*\"sentenc\" + 0.008*\"polic\" + 0.008*\"guilti\"\n",
      "Topic: 2 Word: 0.026*\"trump\" + 0.012*\"australia\" + 0.010*\"world\" + 0.006*\"australian\" + 0.006*\"coronavirus\" + 0.006*\"south\" + 0.006*\"india\" + 0.006*\"test\" + 0.006*\"open\" + 0.006*\"korea\"\n",
      "Topic: 3 Word: 0.017*\"coronavirus\" + 0.014*\"covid\" + 0.010*\"rural\" + 0.007*\"farmer\" + 0.006*\"restrict\" + 0.006*\"australia\" + 0.006*\"farm\" + 0.005*\"nation\" + 0.005*\"govern\" + 0.005*\"news\"\n",
      "Topic: 4 Word: 0.014*\"interview\" + 0.012*\"market\" + 0.010*\"morrison\" + 0.010*\"scott\" + 0.010*\"monday\" + 0.009*\"share\" + 0.008*\"extend\" + 0.006*\"daniel\" + 0.006*\"novemb\" + 0.006*\"australian\"\n",
      "Topic: 5 Word: 0.016*\"crash\" + 0.015*\"polic\" + 0.012*\"drum\" + 0.012*\"woman\" + 0.010*\"death\" + 0.009*\"shoot\" + 0.009*\"miss\" + 0.008*\"dead\" + 0.008*\"die\" + 0.008*\"search\"\n",
      "Topic: 6 Word: 0.022*\"donald\" + 0.017*\"victoria\" + 0.012*\"coronavirus\" + 0.011*\"weather\" + 0.010*\"wednesday\" + 0.010*\"queensland\" + 0.008*\"alan\" + 0.007*\"burn\" + 0.006*\"april\" + 0.006*\"jam\"\n",
      "Topic: 7 Word: 0.015*\"news\" + 0.008*\"lockdown\" + 0.008*\"mental\" + 0.007*\"coronavirus\" + 0.007*\"street\" + 0.007*\"grandstand\" + 0.006*\"leagu\" + 0.006*\"busi\" + 0.006*\"wall\" + 0.006*\"final\"\n",
      "Topic: 8 Word: 0.019*\"countri\" + 0.014*\"hour\" + 0.013*\"royal\" + 0.010*\"commiss\" + 0.007*\"social\" + 0.007*\"know\" + 0.007*\"explain\" + 0.007*\"coronavirus\" + 0.006*\"parent\" + 0.006*\"morn\"\n",
      "Topic: 9 Word: 0.014*\"elect\" + 0.008*\"govern\" + 0.008*\"andrew\" + 0.007*\"labor\" + 0.007*\"michael\" + 0.007*\"financ\" + 0.007*\"liber\" + 0.006*\"peter\" + 0.006*\"say\" + 0.005*\"parti\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, \n",
    "                                             passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation:  Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4136901795864105\t \n",
      "Topic: 0.029*\"govern\" + 0.019*\"health\" + 0.019*\"school\" + 0.017*\"help\" + 0.016*\"chang\" + 0.015*\"feder\" + 0.013*\"indigen\" + 0.012*\"state\" + 0.012*\"coronavirus\" + 0.012*\"fund\"\n",
      "\n",
      "Score: 0.34723326563835144\t \n",
      "Topic: 0.025*\"call\" + 0.025*\"tasmania\" + 0.020*\"rise\" + 0.019*\"victorian\" + 0.017*\"morrison\" + 0.017*\"tasmanian\" + 0.015*\"million\" + 0.015*\"farm\" + 0.011*\"program\" + 0.011*\"claim\"\n",
      "\n",
      "Score: 0.15154524147510529\t \n",
      "Topic: 0.070*\"australia\" + 0.045*\"trump\" + 0.025*\"donald\" + 0.017*\"elect\" + 0.016*\"border\" + 0.015*\"busi\" + 0.015*\"peopl\" + 0.014*\"accus\" + 0.013*\"say\" + 0.012*\"scott\"\n",
      "\n",
      "Score: 0.012506252154707909\t \n",
      "Topic: 0.051*\"coronavirus\" + 0.028*\"covid\" + 0.024*\"live\" + 0.021*\"nation\" + 0.021*\"coast\" + 0.016*\"restrict\" + 0.015*\"water\" + 0.013*\"gold\" + 0.011*\"plan\" + 0.010*\"park\"\n",
      "\n",
      "Score: 0.01250444259494543\t \n",
      "Topic: 0.037*\"year\" + 0.031*\"melbourn\" + 0.022*\"open\" + 0.021*\"canberra\" + 0.017*\"jail\" + 0.015*\"work\" + 0.014*\"high\" + 0.014*\"life\" + 0.013*\"interview\" + 0.013*\"offic\"\n",
      "\n",
      "Score: 0.012504381127655506\t \n",
      "Topic: 0.030*\"china\" + 0.025*\"test\" + 0.019*\"south\" + 0.016*\"coronavirus\" + 0.013*\"north\" + 0.012*\"australian\" + 0.012*\"rural\" + 0.011*\"presid\" + 0.011*\"train\" + 0.011*\"minist\"\n",
      "\n",
      "Score: 0.012504227459430695\t \n",
      "Topic: 0.042*\"queensland\" + 0.033*\"victoria\" + 0.022*\"bushfir\" + 0.022*\"hous\" + 0.014*\"time\" + 0.013*\"royal\" + 0.013*\"west\" + 0.011*\"guilti\" + 0.011*\"commiss\" + 0.011*\"farmer\"\n",
      "\n",
      "Score: 0.012503993697464466\t \n",
      "Topic: 0.026*\"death\" + 0.025*\"charg\" + 0.025*\"case\" + 0.024*\"court\" + 0.021*\"polic\" + 0.020*\"murder\" + 0.015*\"alleg\" + 0.012*\"trial\" + 0.012*\"arrest\" + 0.012*\"face\"\n",
      "\n",
      "Score: 0.012503993697464466\t \n",
      "Topic: 0.022*\"news\" + 0.020*\"market\" + 0.018*\"world\" + 0.017*\"women\" + 0.015*\"final\" + 0.015*\"australian\" + 0.014*\"island\" + 0.012*\"return\" + 0.011*\"street\" + 0.011*\"fall\"\n",
      "\n",
      "Score: 0.012503993697464466\t \n",
      "Topic: 0.038*\"sydney\" + 0.025*\"polic\" + 0.021*\"crash\" + 0.020*\"adelaid\" + 0.019*\"die\" + 0.015*\"miss\" + 0.012*\"break\" + 0.011*\"drug\" + 0.011*\"driver\" + 0.010*\"road\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.715409517288208\t \n",
      "Topic: 0.014*\"elect\" + 0.008*\"govern\" + 0.008*\"andrew\" + 0.007*\"labor\" + 0.007*\"michael\" + 0.007*\"financ\" + 0.007*\"liber\" + 0.006*\"peter\" + 0.006*\"say\" + 0.005*\"parti\"\n",
      "\n",
      "Score: 0.18454216420650482\t \n",
      "Topic: 0.019*\"countri\" + 0.014*\"hour\" + 0.013*\"royal\" + 0.010*\"commiss\" + 0.007*\"social\" + 0.007*\"know\" + 0.007*\"explain\" + 0.007*\"coronavirus\" + 0.006*\"parent\" + 0.006*\"morn\"\n",
      "\n",
      "Score: 0.012509500607848167\t \n",
      "Topic: 0.017*\"coronavirus\" + 0.014*\"covid\" + 0.010*\"rural\" + 0.007*\"farmer\" + 0.006*\"restrict\" + 0.006*\"australia\" + 0.006*\"farm\" + 0.005*\"nation\" + 0.005*\"govern\" + 0.005*\"news\"\n",
      "\n",
      "Score: 0.012505966238677502\t \n",
      "Topic: 0.010*\"christma\" + 0.010*\"stori\" + 0.009*\"sport\" + 0.007*\"violenc\" + 0.007*\"domest\" + 0.007*\"award\" + 0.006*\"septemb\" + 0.006*\"celebr\" + 0.006*\"australian\" + 0.006*\"dollar\"\n",
      "\n",
      "Score: 0.01250587310642004\t \n",
      "Topic: 0.014*\"interview\" + 0.012*\"market\" + 0.010*\"morrison\" + 0.010*\"scott\" + 0.010*\"monday\" + 0.009*\"share\" + 0.008*\"extend\" + 0.006*\"daniel\" + 0.006*\"novemb\" + 0.006*\"australian\"\n",
      "\n",
      "Score: 0.01250558253377676\t \n",
      "Topic: 0.015*\"news\" + 0.008*\"lockdown\" + 0.008*\"mental\" + 0.007*\"coronavirus\" + 0.007*\"street\" + 0.007*\"grandstand\" + 0.006*\"leagu\" + 0.006*\"busi\" + 0.006*\"wall\" + 0.006*\"final\"\n",
      "\n",
      "Score: 0.012505517341196537\t \n",
      "Topic: 0.022*\"donald\" + 0.017*\"victoria\" + 0.012*\"coronavirus\" + 0.011*\"weather\" + 0.010*\"wednesday\" + 0.010*\"queensland\" + 0.008*\"alan\" + 0.007*\"burn\" + 0.006*\"april\" + 0.006*\"jam\"\n",
      "\n",
      "Score: 0.012505456805229187\t \n",
      "Topic: 0.014*\"murder\" + 0.013*\"court\" + 0.012*\"charg\" + 0.010*\"alleg\" + 0.008*\"friday\" + 0.008*\"jail\" + 0.008*\"drug\" + 0.008*\"sentenc\" + 0.008*\"polic\" + 0.008*\"guilti\"\n",
      "\n",
      "Score: 0.012505450285971165\t \n",
      "Topic: 0.026*\"trump\" + 0.012*\"australia\" + 0.010*\"world\" + 0.006*\"australian\" + 0.006*\"coronavirus\" + 0.006*\"south\" + 0.006*\"india\" + 0.006*\"test\" + 0.006*\"open\" + 0.006*\"korea\"\n",
      "\n",
      "Score: 0.012504949234426022\t \n",
      "Topic: 0.016*\"crash\" + 0.015*\"polic\" + 0.012*\"drum\" + 0.012*\"woman\" + 0.010*\"death\" + 0.009*\"shoot\" + 0.009*\"miss\" + 0.008*\"dead\" + 0.008*\"die\" + 0.008*\"search\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.34983640909194946\t Topic: 0.026*\"death\" + 0.025*\"charg\" + 0.025*\"case\" + 0.024*\"court\" + 0.021*\"polic\"\n",
      "Score: 0.3497518002986908\t Topic: 0.022*\"news\" + 0.020*\"market\" + 0.018*\"world\" + 0.017*\"women\" + 0.015*\"final\"\n",
      "Score: 0.18368104100227356\t Topic: 0.042*\"queensland\" + 0.033*\"victoria\" + 0.022*\"bushfir\" + 0.022*\"hous\" + 0.014*\"time\"\n",
      "Score: 0.016682041808962822\t Topic: 0.030*\"china\" + 0.025*\"test\" + 0.019*\"south\" + 0.016*\"coronavirus\" + 0.013*\"north\"\n",
      "Score: 0.016676753759384155\t Topic: 0.025*\"call\" + 0.025*\"tasmania\" + 0.020*\"rise\" + 0.019*\"victorian\" + 0.017*\"morrison\"\n",
      "Score: 0.016675986349582672\t Topic: 0.051*\"coronavirus\" + 0.028*\"covid\" + 0.024*\"live\" + 0.021*\"nation\" + 0.021*\"coast\"\n",
      "Score: 0.016675952821969986\t Topic: 0.029*\"govern\" + 0.019*\"health\" + 0.019*\"school\" + 0.017*\"help\" + 0.016*\"chang\"\n",
      "Score: 0.016673337668180466\t Topic: 0.038*\"sydney\" + 0.025*\"polic\" + 0.021*\"crash\" + 0.020*\"adelaid\" + 0.019*\"die\"\n",
      "Score: 0.016673337668180466\t Topic: 0.037*\"year\" + 0.031*\"melbourn\" + 0.022*\"open\" + 0.021*\"canberra\" + 0.017*\"jail\"\n",
      "Score: 0.016673337668180466\t Topic: 0.070*\"australia\" + 0.045*\"trump\" + 0.025*\"donald\" + 0.017*\"elect\" + 0.016*\"border\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
